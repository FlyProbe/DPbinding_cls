{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "from models import classifier_v3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "tf_data_v4 = torch.load('../embedding/training_set_tf_embedding_v4.pt')\n",
    "dna_data_v4 = torch.load('../embedding/training_set_DNA_embedding_v4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35715, 960]), 35715, torch.Size([768]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_v4.shape, len(dna_data_v4), dna_data_v4[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.7420e-03, -5.9488e-03, -7.2344e-03,  ..., -4.7327e-03,\n",
       "         -1.4604e-02, -1.5792e-02],\n",
       "        [-3.3671e-03,  7.8256e-03, -2.2358e-03,  ...,  1.2239e-03,\n",
       "         -2.0149e-02, -1.0832e-02],\n",
       "        [ 1.1129e-02, -1.5702e-02, -2.6762e-03,  ...,  1.0946e-02,\n",
       "         -1.7149e-02, -1.4492e-02],\n",
       "        ...,\n",
       "        [-7.5461e-05, -1.2941e-02,  1.3136e-02,  ..., -1.4155e-02,\n",
       "         -3.1340e-02,  1.1025e-02],\n",
       "        [-7.5706e-03, -8.7105e-05, -1.4696e-02,  ..., -4.4630e-03,\n",
       "         -2.2061e-02, -2.7349e-02],\n",
       "        [ 4.3478e-03, -1.8013e-02, -1.6710e-02,  ...,  1.2651e-03,\n",
       "         -1.6302e-02, -1.4976e-02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_v4.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNA tensor shape: torch.Size([35715, 768])\n"
     ]
    }
   ],
   "source": [
    "# Get the dimensions of the first tensor to determine shape\n",
    "feature_dim = dna_data_v4[0].size(0)\n",
    "num_samples = len(dna_data_v4)\n",
    "\n",
    "# Create a new tensor with the appropriate dimensions\n",
    "dna_tensor = torch.zeros((num_samples, feature_dim))\n",
    "\n",
    "# Copy data from each tensor in the list to the new tensor\n",
    "for i, tensor in enumerate(dna_data_v4):\n",
    "    dna_tensor[i] = tensor\n",
    "\n",
    "# Verify shape\n",
    "print(f\"DNA tensor shape: {dna_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#get labels\n",
    "labels = pd.read_csv('../dataset/training_dataset_with_negatives_v4.csv')['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data V3: new training set with pos:neg = 1:2, negtive samples are generatedby shuffling across different species\n",
    "\n",
    "# Model R1\n",
    "# === Cross Validation Mean Accuracy: 91.98% ===\n",
    "# === Cross Validation Mean ROC AUC: 0.9623 ===\n",
    "# === Overall ROC AUC (all folds combined): 0.9599 ===\n",
    "\n",
    "# Model R2: removed feature extraction layer before cross attention\n",
    "# === Cross Validation Mean Accuracy: 90.46% ===\n",
    "# === Cross Validation Mean ROC AUC: 0.9458 ===\n",
    "# === Overall ROC AUC (all folds combined): 0.9439 ===\n",
    "\n",
    "# Data v4: clustering based on proteins to split training and test set\n",
    "\n",
    "# === Cross Validation Mean Accuracy: 90.32% ===\n",
    "# === Cross Validation Mean ROC AUC: 0.9486 ===\n",
    "# === Overall ROC AUC (all folds combined): 0.9459 ===\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Fold 1/5 ====\n",
      "Epoch 1/20, Loss: 0.5747, LR: 0.000000\n",
      "Epoch 2/20, Loss: 0.4428, LR: 0.000010\n",
      "Epoch 3/20, Loss: 0.3861, LR: 0.000010\n",
      "Epoch 4/20, Loss: 0.3518, LR: 0.000010\n",
      "Epoch 5/20, Loss: 0.3266, LR: 0.000010\n",
      "Epoch 6/20, Loss: 0.3067, LR: 0.000010\n",
      "Epoch 7/20, Loss: 0.2848, LR: 0.000010\n",
      "Epoch 8/20, Loss: 0.2649, LR: 0.000010\n",
      "Epoch 9/20, Loss: 0.2501, LR: 0.000010\n",
      "Epoch 10/20, Loss: 0.2341, LR: 0.000010\n",
      "Epoch 11/20, Loss: 0.2175, LR: 0.000010\n",
      "Epoch 12/20, Loss: 0.2049, LR: 0.000010\n",
      "Epoch 13/20, Loss: 0.1946, LR: 0.000010\n",
      "Epoch 14/20, Loss: 0.1837, LR: 0.000010\n",
      "Epoch 15/20, Loss: 0.1732, LR: 0.000010\n",
      "Epoch 16/20, Loss: 0.1627, LR: 0.000010\n",
      "Epoch 17/20, Loss: 0.1528, LR: 0.000010\n",
      "Epoch 18/20, Loss: 0.1445, LR: 0.000010\n",
      "Epoch 19/20, Loss: 0.1416, LR: 0.000010\n",
      "Epoch 20/20, Loss: 0.1288, LR: 0.000010\n",
      "Validation Accuracy (Fold 1): 89.51%\n",
      "ROC AUC Score (Fold 1): 0.9464\n",
      "\n",
      "==== Fold 2/5 ====\n",
      "Epoch 1/20, Loss: 0.5736, LR: 0.000000\n",
      "Epoch 2/20, Loss: 0.4445, LR: 0.000010\n",
      "Epoch 3/20, Loss: 0.3904, LR: 0.000010\n",
      "Epoch 4/20, Loss: 0.3569, LR: 0.000010\n",
      "Epoch 5/20, Loss: 0.3341, LR: 0.000010\n",
      "Epoch 6/20, Loss: 0.3115, LR: 0.000010\n",
      "Epoch 7/20, Loss: 0.2918, LR: 0.000010\n",
      "Epoch 8/20, Loss: 0.2747, LR: 0.000010\n",
      "Epoch 9/20, Loss: 0.2568, LR: 0.000010\n",
      "Epoch 10/20, Loss: 0.2445, LR: 0.000010\n",
      "Epoch 11/20, Loss: 0.2280, LR: 0.000010\n",
      "Epoch 12/20, Loss: 0.2155, LR: 0.000010\n",
      "Epoch 13/20, Loss: 0.1996, LR: 0.000010\n",
      "Epoch 14/20, Loss: 0.1869, LR: 0.000010\n",
      "Epoch 15/20, Loss: 0.1763, LR: 0.000010\n",
      "Epoch 16/20, Loss: 0.1665, LR: 0.000010\n",
      "Epoch 17/20, Loss: 0.1590, LR: 0.000010\n",
      "Epoch 18/20, Loss: 0.1508, LR: 0.000010\n",
      "Epoch 19/20, Loss: 0.1409, LR: 0.000010\n",
      "Epoch 20/20, Loss: 0.1352, LR: 0.000010\n",
      "Validation Accuracy (Fold 2): 90.47%\n",
      "ROC AUC Score (Fold 2): 0.9470\n",
      "\n",
      "==== Fold 3/5 ====\n",
      "Epoch 1/20, Loss: 0.5686, LR: 0.000000\n",
      "Epoch 2/20, Loss: 0.4367, LR: 0.000010\n",
      "Epoch 3/20, Loss: 0.3870, LR: 0.000010\n",
      "Epoch 4/20, Loss: 0.3545, LR: 0.000010\n",
      "Epoch 5/20, Loss: 0.3313, LR: 0.000010\n",
      "Epoch 6/20, Loss: 0.3073, LR: 0.000010\n",
      "Epoch 7/20, Loss: 0.2900, LR: 0.000010\n",
      "Epoch 8/20, Loss: 0.2718, LR: 0.000010\n",
      "Epoch 9/20, Loss: 0.2536, LR: 0.000010\n",
      "Epoch 10/20, Loss: 0.2372, LR: 0.000010\n",
      "Epoch 11/20, Loss: 0.2243, LR: 0.000010\n",
      "Epoch 12/20, Loss: 0.2095, LR: 0.000010\n",
      "Epoch 13/20, Loss: 0.1972, LR: 0.000010\n",
      "Epoch 14/20, Loss: 0.1844, LR: 0.000010\n",
      "Epoch 15/20, Loss: 0.1731, LR: 0.000010\n",
      "Epoch 16/20, Loss: 0.1622, LR: 0.000010\n",
      "Epoch 17/20, Loss: 0.1533, LR: 0.000010\n",
      "Epoch 18/20, Loss: 0.1478, LR: 0.000010\n",
      "Epoch 19/20, Loss: 0.1351, LR: 0.000010\n",
      "Epoch 20/20, Loss: 0.1323, LR: 0.000010\n",
      "Validation Accuracy (Fold 3): 91.26%\n",
      "ROC AUC Score (Fold 3): 0.9539\n",
      "\n",
      "==== Fold 4/5 ====\n",
      "Epoch 1/20, Loss: 0.5752, LR: 0.000000\n",
      "Epoch 2/20, Loss: 0.4384, LR: 0.000010\n",
      "Epoch 3/20, Loss: 0.3833, LR: 0.000010\n",
      "Epoch 4/20, Loss: 0.3488, LR: 0.000010\n",
      "Epoch 5/20, Loss: 0.3258, LR: 0.000010\n",
      "Epoch 6/20, Loss: 0.3016, LR: 0.000010\n",
      "Epoch 7/20, Loss: 0.2811, LR: 0.000010\n",
      "Epoch 8/20, Loss: 0.2656, LR: 0.000010\n",
      "Epoch 9/20, Loss: 0.2453, LR: 0.000010\n",
      "Epoch 10/20, Loss: 0.2342, LR: 0.000010\n",
      "Epoch 11/20, Loss: 0.2181, LR: 0.000010\n",
      "Epoch 12/20, Loss: 0.2070, LR: 0.000010\n",
      "Epoch 13/20, Loss: 0.1907, LR: 0.000010\n",
      "Epoch 14/20, Loss: 0.1798, LR: 0.000010\n",
      "Epoch 15/20, Loss: 0.1709, LR: 0.000010\n",
      "Epoch 16/20, Loss: 0.1609, LR: 0.000010\n",
      "Epoch 17/20, Loss: 0.1512, LR: 0.000010\n",
      "Epoch 18/20, Loss: 0.1424, LR: 0.000010\n",
      "Epoch 19/20, Loss: 0.1372, LR: 0.000010\n",
      "Epoch 20/20, Loss: 0.1314, LR: 0.000010\n",
      "Validation Accuracy (Fold 4): 89.60%\n",
      "ROC AUC Score (Fold 4): 0.9471\n",
      "\n",
      "==== Fold 5/5 ====\n",
      "Epoch 1/20, Loss: 0.5744, LR: 0.000000\n",
      "Epoch 2/20, Loss: 0.4337, LR: 0.000010\n",
      "Epoch 3/20, Loss: 0.3814, LR: 0.000010\n",
      "Epoch 4/20, Loss: 0.3479, LR: 0.000010\n",
      "Epoch 5/20, Loss: 0.3230, LR: 0.000010\n",
      "Epoch 6/20, Loss: 0.3014, LR: 0.000010\n",
      "Epoch 7/20, Loss: 0.2845, LR: 0.000010\n",
      "Epoch 8/20, Loss: 0.2668, LR: 0.000010\n",
      "Epoch 9/20, Loss: 0.2493, LR: 0.000010\n",
      "Epoch 10/20, Loss: 0.2349, LR: 0.000010\n",
      "Epoch 11/20, Loss: 0.2227, LR: 0.000010\n",
      "Epoch 12/20, Loss: 0.2083, LR: 0.000010\n",
      "Epoch 13/20, Loss: 0.1967, LR: 0.000010\n",
      "Epoch 14/20, Loss: 0.1848, LR: 0.000010\n",
      "Epoch 15/20, Loss: 0.1736, LR: 0.000010\n",
      "Epoch 16/20, Loss: 0.1647, LR: 0.000010\n",
      "Epoch 17/20, Loss: 0.1568, LR: 0.000010\n",
      "Epoch 18/20, Loss: 0.1487, LR: 0.000010\n",
      "Epoch 19/20, Loss: 0.1390, LR: 0.000010\n",
      "Epoch 20/20, Loss: 0.1349, LR: 0.000010\n",
      "Validation Accuracy (Fold 5): 90.75%\n",
      "ROC AUC Score (Fold 5): 0.9485\n",
      "\n",
      "=== Cross Validation Mean Accuracy: 90.32% ===\n",
      "=== Cross Validation Mean ROC AUC: 0.9486 ===\n",
      "=== Overall ROC AUC (all folds combined): 0.9459 ===\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 交叉验证参数\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 记录每折的性能\n",
    "fold_results = []\n",
    "fold_auc_scores = []\n",
    "all_true_labels = []\n",
    "all_pred_probs = []\n",
    "\n",
    "# Learning rate warmup parameters\n",
    "warmup_steps = 100  # Number of iterations for warmup\n",
    "total_steps = 0  # Will be calculated based on epochs and batches\n",
    "\n",
    "# 交叉验证循环\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dna_tensor)):\n",
    "    print(f\"\\n==== Fold {fold+1}/{num_folds} ====\")\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    dna_train, dna_val = dna_tensor[train_idx], dna_tensor[val_idx]\n",
    "    protein_train, protein_val = tf_data_v4[train_idx], tf_data_v4[val_idx]\n",
    "\n",
    "    # Convert labels pandas Series to tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "    labels_train, labels_val = labels_tensor[train_idx], labels_tensor[val_idx]\n",
    "\n",
    "    # 转换成 DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(dna_train, protein_train, labels_train)\n",
    "    val_dataset = torch.utils.data.TensorDataset(dna_val, protein_val, labels_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 初始化模型\n",
    "    classifier = model.DNAProteinClassifier().to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=1e-5)\n",
    "\n",
    "    # Calculate total steps for the scheduler\n",
    "    num_epochs = 20\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "\n",
    "    # Define warmup function\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return 1.0\n",
    "\n",
    "    # Create scheduler with warmup\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        for dna_batch, protein_batch, label_batch in train_loader:\n",
    "            dna_batch, protein_batch, label_batch = dna_batch.to(device), protein_batch.to(device), label_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(dna_batch, protein_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    # 评估模型\n",
    "    classifier.eval()\n",
    "    correct, total = 0, 0\n",
    "    true_labels = []\n",
    "    pred_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for dna_batch, protein_batch, label_batch in val_loader:\n",
    "            dna_batch, protein_batch, label_batch = dna_batch.to(device), protein_batch.to(device), label_batch.to(device)\n",
    "            outputs = classifier(dna_batch, protein_batch)\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == label_batch).sum().item()\n",
    "            total += label_batch.size(0)\n",
    "\n",
    "            # Collect true labels and prediction probabilities for ROC AUC calculation\n",
    "            true_labels.extend(label_batch.cpu().numpy())\n",
    "            pred_probs.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total\n",
    "    auc_score = roc_auc_score(true_labels, pred_probs)\n",
    "\n",
    "    print(f\"Validation Accuracy (Fold {fold+1}): {accuracy * 100:.2f}%\")\n",
    "    print(f\"ROC AUC Score (Fold {fold+1}): {auc_score:.4f}\")\n",
    "\n",
    "    fold_results.append(accuracy)\n",
    "    fold_auc_scores.append(auc_score)\n",
    "\n",
    "    # Store for overall ROC AUC\n",
    "    all_true_labels.extend(true_labels)\n",
    "    all_pred_probs.extend(pred_probs)\n",
    "\n",
    "# 计算平均准确率和AUC\n",
    "print(f\"\\n=== Cross Validation Mean Accuracy: {np.mean(fold_results) * 100:.2f}% ===\")\n",
    "print(f\"=== Cross Validation Mean ROC AUC: {np.mean(fold_auc_scores):.4f} ===\")\n",
    "\n",
    "# Calculate overall ROC AUC from all folds combined\n",
    "overall_auc = roc_auc_score(all_true_labels, all_pred_probs)\n",
    "print(f\"=== Overall ROC AUC (all folds combined): {overall_auc:.4f} ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models_v3/dna_protein_classifier_v4r1.pth\n",
      "Full model saved to ../models_v3/dna_protein_classifier_full_v4r1.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the model to a local file\n",
    "model_path = '../models_v3/dna_protein_classifier_v4r1.pth'\n",
    "torch.save(classifier.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# To save the entire model (not just state_dict)\n",
    "full_model_path = '../models_v3/dna_protein_classifier_full_v4r1.pt'\n",
    "torch.save(classifier, full_model_path)\n",
    "print(f\"Full model saved to {full_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Structure:\n",
      "DNAProteinClassifier(\n",
      "  (dna_feature_extractor): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (protein_feature_extractor): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (1): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (bi_cross_attn): BiCrossAttention(\n",
      "    (dna_proj): Linear(in_features=768, out_features=960, bias=True)\n",
      "    (cross_attn_dna): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=960, out_features=960, bias=True)\n",
      "    )\n",
      "    (cross_attn_protein): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=960, out_features=960, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (pool): PoolingLayer()\n",
      "  (self_attn1): SelfAttentionBlock(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1920, out_features=1920, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (res_block1): ResNetBlock(\n",
      "    (fc1): Linear(in_features=1920, out_features=960, bias=True)\n",
      "    (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (bn2): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=960, out_features=1920, bias=True)\n",
      "    (bn3): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (self_attn2): SelfAttentionBlock(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1920, out_features=1920, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (res_block2): ResNetBlock(\n",
      "    (fc1): Linear(in_features=1920, out_features=960, bias=True)\n",
      "    (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (bn2): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=960, out_features=1920, bias=True)\n",
      "    (bn3): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (self_attn3): SelfAttentionBlock(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1920, out_features=1920, bias=True)\n",
      "    )\n",
      "    (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (res_block3): ResNetBlock(\n",
      "    (fc1): Linear(in_features=1920, out_features=960, bias=True)\n",
      "    (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc2): Linear(in_features=960, out_features=960, bias=True)\n",
      "    (bn2): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fc3): Linear(in_features=960, out_features=1920, bias=True)\n",
      "    (bn3): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (pool3): PoolingLayer()\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1920, out_features=768, bias=True)\n",
      "    (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=768, out_features=384, bias=True)\n",
      "    (5): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=384, out_features=96, bias=True)\n",
      "    (9): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=96, out_features=1, bias=True)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 69,575,425\n",
      "Trainable parameters: 69,575,425\n",
      "\n",
      "dna_feature_extractor:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "protein_feature_extractor:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=960, out_features=960, bias=True)\n",
      "  (1): LayerNorm((960,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "bi_cross_attn:\n",
      "BiCrossAttention(\n",
      "  (dna_proj): Linear(in_features=768, out_features=960, bias=True)\n",
      "  (cross_attn_dna): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=960, out_features=960, bias=True)\n",
      "  )\n",
      "  (cross_attn_protein): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=960, out_features=960, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "pool:\n",
      "PoolingLayer()\n",
      "\n",
      "self_attn1:\n",
      "SelfAttentionBlock(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1920, out_features=1920, bias=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "res_block1:\n",
      "ResNetBlock(\n",
      "  (fc1): Linear(in_features=1920, out_features=960, bias=True)\n",
      "  (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=960, out_features=960, bias=True)\n",
      "  (bn2): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=960, out_features=1920, bias=True)\n",
      "  (bn3): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "self_attn2:\n",
      "SelfAttentionBlock(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1920, out_features=1920, bias=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "res_block2:\n",
      "ResNetBlock(\n",
      "  (fc1): Linear(in_features=1920, out_features=960, bias=True)\n",
      "  (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=960, out_features=960, bias=True)\n",
      "  (bn2): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=960, out_features=1920, bias=True)\n",
      "  (bn3): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "self_attn3:\n",
      "SelfAttentionBlock(\n",
      "  (self_attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=1920, out_features=1920, bias=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1920,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "\n",
      "res_block3:\n",
      "ResNetBlock(\n",
      "  (fc1): Linear(in_features=1920, out_features=960, bias=True)\n",
      "  (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=960, out_features=960, bias=True)\n",
      "  (bn2): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=960, out_features=1920, bias=True)\n",
      "  (bn3): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      ")\n",
      "\n",
      "pool3:\n",
      "PoolingLayer()\n",
      "\n",
      "fc:\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1920, out_features=768, bias=True)\n",
      "  (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.3, inplace=False)\n",
      "  (4): Linear(in_features=768, out_features=384, bias=True)\n",
      "  (5): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.2, inplace=False)\n",
      "  (8): Linear(in_features=384, out_features=96, bias=True)\n",
      "  (9): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (10): ReLU()\n",
      "  (11): Linear(in_features=96, out_features=1, bias=True)\n",
      "  (12): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = model.DNAProteinClassifier()\n",
    "# Print the model structure\n",
    "print(\"Model Structure:\")\n",
    "print(classifier)\n",
    "\n",
    "# Get the number of parameters in the model\n",
    "total_params = sum(p.numel() for p in classifier.parameters())\n",
    "trainable_params = sum(p.numel() for p in classifier.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Print the model's layer types and sizes\n",
    "for name, module in classifier.named_children():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
